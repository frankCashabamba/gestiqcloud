# ü§ñ Implementaci√≥n Completa de IA en GestiqCloud

## üìã Resumen Ejecutivo

Se ha implementado una **arquitectura centralizada y extensible de IA** que permite:

‚úÖ **Desarrollo**: Usar Ollama local (gratuito, privado)  
‚úÖ **Producci√≥n**: Usar OVHCloud AI (empresarial)  
‚úÖ **Fallback**: OpenAI como respaldo autom√°tico  
‚úÖ **M√∫ltiples tareas**: Clasificaci√≥n, an√°lisis, generaci√≥n, chat, sugerencias  
‚úÖ **Sin dependencias nuevas**: Solo usa httpx (ya en requirements.txt)  

## üèóÔ∏è Estructura Implementada

```
apps/backend/app/services/ai/
‚îú‚îÄ‚îÄ __init__.py                      # Exports p√∫blicos
‚îú‚îÄ‚îÄ base.py                          # Interface base + tipos
‚îú‚îÄ‚îÄ service.py                       # API unificada de alto nivel
‚îú‚îÄ‚îÄ factory.py                       # Factory y gesti√≥n de proveedores
‚îî‚îÄ‚îÄ providers/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ ollama.py                   # Proveedor Ollama (local)
    ‚îú‚îÄ‚îÄ ovhcloud.py                 # Proveedor OVHCloud (producci√≥n)
    ‚îî‚îÄ‚îÄ openai.py                   # Proveedor OpenAI (fallback)

apps/backend/app/routers/
‚îî‚îÄ‚îÄ ai_health.py                    # Endpoints de health check

Documentation/
‚îú‚îÄ‚îÄ AI_INTEGRATION_GUIDE.md         # Gu√≠a completa de uso
‚îú‚îÄ‚îÄ COPILOT_ENHANCEMENT.md          # Plan de mejora del Copilot
‚îú‚îÄ‚îÄ .env.ai.example                 # Configuraci√≥n de ejemplo
‚îî‚îÄ‚îÄ IA_IMPLEMENTATION_SUMMARY.md    # Este archivo
```

## üéØ Caracter√≠sticas Principales

### 1. Abstracci√≥n de Proveedores
```python
# Mismo c√≥digo, diferentes proveedores
response = await AIService.query(
    task=AITask.ANALYSIS,
    prompt="Tu prompt aqu√≠"
)
# En dev: usa Ollama local
# En prod: usa OVHCloud
# Si falla: intenta OpenAI
```

### 2. Tipos de Tareas Soportadas
- **CLASSIFICATION**: Clasificar documentos
- **ANALYSIS**: Analizar datos e incidencias
- **GENERATION**: Generar documentos (facturas, √≥rdenes)
- **SUGGESTION**: Sugerencias contextuales
- **CHAT**: Conversaci√≥n general
- **EXTRACTION**: Extracci√≥n de datos

### 3. API de Alto Nivel
```python
from app.services.ai import AIService

# Clasificar documento
result = await AIService.classify_document(content, types)

# Analizar incidencia
analysis = await AIService.analyze_incident(type, description)

# Generar sugerencias
suggestion = await AIService.generate_suggestion(context)

# Generar borrador
draft = await AIService.generate_document_draft(type, data)
```

## üöÄ Quick Start en 5 Minutos

### Desarrollo Local

1. **Instalar Ollama**
```bash
curl https://ollama.ai/install.sh | sh
ollama pull llama3.1:8b
ollama serve
```

2. **Configurar .env**
```bash
ENVIRONMENT=development
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
```

3. **Usar en c√≥digo**
```python
from app.services.ai import AIService, AITask

response = await AIService.query(
    task=AITask.ANALYSIS,
    prompt="Analiza estos datos..."
)
```

4. **Listo** ‚úÖ

### Producci√≥n

1. **Configurar credenciales OVHCloud**
```bash
ENVIRONMENT=production
OVHCLOUD_API_KEY=xxx
OVHCLOUD_API_SECRET=xxx
OPENAI_API_KEY=xxx  # Fallback
```

2. **Health check**
```bash
curl http://api.example.com/api/v1/health/ai
```

## üìä Integraci√≥n en M√≥dulos

### Copilot (Inmediato)
```python
# Mejorar queries con an√°lisis IA
result = await query_readonly_enhanced(db, "ventas_mes")
# Retorna: datos + ai_insights

# Generar sugerencias autom√°ticas
suggestions = await get_smart_suggestions(db)
```

### Imports (Fase 2)
```python
# Clasificar documentos autom√°ticamente
result = await AIService.classify_document(text, types)
if result['requires_review']:
    send_for_manual_review()
```

### Incidents (Fase 2)
```python
# Analizar incidencias
analysis = await AIService.analyze_incident(
    incident_type, description, stack_trace
)
incident.ai_analysis = analysis
```

### Chat (Fase 3)
```python
# Conversaci√≥n inteligente
response = await AIService.query(
    task=AITask.CHAT,
    prompt=user_message,
    context=module_data
)
```

## ‚öôÔ∏è Configuraci√≥n

### Variables Obligatorias

**Desarrollo:**
```bash
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
```

**Producci√≥n:**
```bash
OVHCLOUD_API_KEY=...
OVHCLOUD_API_SECRET=...
OPENAI_API_KEY=...  # Fallback recomendado
```

### Variables Opcionales
```bash
# L√≠mites
AI_MAX_PROMPT_LENGTH=10000
OLLAMA_TIMEOUT=30
OVHCLOUD_TIMEOUT=60

# Modelos espec√≠ficos
OLLAMA_MODEL=llama3.1:8b
OVHCLOUD_MODEL=gpt-4o
OPENAI_MODEL=gpt-3.5-turbo
```

Ver: `apps/backend/.env.ai.example`

## üîê Seguridad

### Privacidad de Datos
- ‚úÖ **Ollama**: Completamente local, sin datos externos
- ‚úÖ **OVHCloud**: Encriptado, cumple GDPR
- ‚ö†Ô∏è **OpenAI**: Revisar t√©rminos antes de usar en producci√≥n

### Validaci√≥n
- M√°ximo prompt: 10,000 caracteres
- Sanitizaci√≥n autom√°tica
- Rate limiting en endpoints (en middleware)

### Rate Limiting
```python
# Implementado en app/middleware/rate_limit.py
# Por defecto: 1000 requests/min en dev, 120 en prod
```

## üìà Monitoreo

### Health Check
```bash
GET /api/v1/health/ai
```

Retorna:
```json
{
  "status": "healthy|degraded|unavailable",
  "primary_provider": "ollama",
  "providers": {
    "ollama": true,
    "ovhcloud": false,
    "openai": true
  }
}
```

### Logs
```python
import logging
logging.getLogger("app.services.ai").setLevel(logging.DEBUG)
```

## üí° Ejemplos de Uso

### Ejemplo 1: An√°lisis de Datos
```python
response = await AIService.query(
    task=AITask.ANALYSIS,
    prompt="Analiza ventas de esta semana y genera insights",
    temperature=0.3,
    max_tokens=1000
)
print(response.content)
```

### Ejemplo 2: Clasificaci√≥n
```python
result = await AIService.classify_document(
    document_content="FACTURA #001...",
    expected_types=["invoice", "receipt", "order"],
    confidence_threshold=0.75
)

if not result['requires_review']:
    process_document(result['type'])
```

### Ejemplo 3: Sugerencia
```python
suggestion = await AIService.generate_suggestion(
    context="Stock bajo, producto con tendencia creciente",
    suggestion_type="inventory"
)
print(suggestion)  # "Considerar aumentar pedido a..."
```

### Ejemplo 4: An√°lisis de Incidencia
```python
analysis = await AIService.analyze_incident(
    incident_type="database_error",
    description="Connection timeout",
    additional_context={"frequency": "intermittent"}
)

print(f"Causa: {analysis['root_cause']}")
print(f"Acciones: {analysis['recommended_actions']}")
```

## üìö Documentaci√≥n Completa

| Documento | Contenido |
|-----------|----------|
| `AI_INTEGRATION_GUIDE.md` | Gu√≠a completa, ejemplos, configuraci√≥n |
| `COPILOT_ENHANCEMENT.md` | Plan de mejora del Copilot en 3 fases |
| `.env.ai.example` | Todas las variables de configuraci√≥n |

## üß™ Testing

### Mock Provider
```python
@pytest.mark.asyncio
async def test_ai_service():
    response = await AIService.query(
        task=AITask.CLASSIFICATION,
        prompt="Test"
    )
    assert not response.is_error
```

### Health Check
```python
from app.services.ai import AIProviderFactory

status = await AIProviderFactory.health_check_all()
# {'ollama': True, 'ovhcloud': False, 'openai': True}
```

## ‚úÖ Checklist de Implementaci√≥n

### Fase 1: Core (Completada)
- ‚úÖ Factory pattern para proveedores
- ‚úÖ Ollama provider (local)
- ‚úÖ OVHCloud provider (producci√≥n)
- ‚úÖ OpenAI provider (fallback)
- ‚úÖ AIService unificado
- ‚úÖ Health check endpoints
- ‚úÖ Documentaci√≥n completa

### Fase 2: Integraci√≥n Copilot (Pr√≥xima)
- [ ] Integrar en query_readonly()
- [ ] Generar sugerencias autom√°ticas
- [ ] Endpoint /suggestions
- [ ] Actualizar Dashboard
- [ ] Pruebas con Ollama

### Fase 3: Chat (Semana 2)
- [ ] Chat conversacional
- [ ] WebSocket para tiempo real
- [ ] Frontend ChatPanel
- [ ] Persistencia de conversaciones

### Fase 4: An√°lisis Avanzado (Semana 3)
- [ ] Predicci√≥n de tendencias
- [ ] Detecci√≥n de anomal√≠as
- [ ] Alertas inteligentes
- [ ] Exportar insights

## üéì Aprendizaje

### Para usar en nuevo m√≥dulo:
1. Importar `AIService`
2. Crear prompt espec√≠fico de la tarea
3. Llamar `AIService.query()` con task apropiada
4. Parsear JSON si es necesario
5. Manejar `response.is_error`

### Buenas pr√°cticas:
- Usa `temperature=0.1-0.3` para tareas determin√≠sticas
- Usa `temperature=0.5-0.7` para creativas
- Cachea respuestas cuando sea posible
- Siempre maneja errores gracefully
- Usa contexto para mejorar relevancia
- Log en DEBUG para debugging

## üìû Support

Para problemas:
1. Revisa `AI_INTEGRATION_GUIDE.md`
2. Comprueba health: `/api/v1/health/ai`
3. Valida .env con `.env.ai.example`
4. Habilita logs DEBUG
5. Prueba conectividad directa al proveedor

## üéâ Conclusi√≥n

Tienes una **plataforma IA moderna y flexible** que:
- Funciona localmente en desarrollo
- Escala a producci√≥n con OVHCloud
- Falla gracefully con fallback autom√°tico
- Soporta m√∫ltiples tipos de tareas
- Es f√°cil de extender
- No requiere nuevas dependencias

**Siguiente paso**: Integrar en Copilot (ver `COPILOT_ENHANCEMENT.md`)

---

**Fecha**: Febrero 2025  
**Versi√≥n**: 1.0  
**Status**: ‚úÖ Implementaci√≥n Completa
