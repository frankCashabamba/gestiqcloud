# üöÄ Setup Local de IA con Ollama

Gu√≠a paso-a-paso para configurar y usar Ollama localmente en tu m√°quina de desarrollo.

## ‚öôÔ∏è Requisitos Previos

- **CPU**: Moderno (Core i5+, M1+, Ryzen 5+)
- **RAM**: M√≠nimo 8GB (16GB recomendado)
- **Espacio disco**: 5-20GB seg√∫n modelo
- **SO**: Windows, macOS, Linux
- **Docker** (opcional, pero recomendado)

## üì¶ Instalaci√≥n

### Windows 10/11

#### Opci√≥n 1: Instalador (Recomendado)
1. Descarga: https://ollama.ai/download/windows
2. Ejecuta el instalador (.exe)
3. Sigue pasos de instalaci√≥n
4. Se iniciar√° autom√°ticamente como servicio
5. Verifica: Ollama deber√≠a estar en system tray

#### Opci√≥n 2: WSL2 + Linux
```bash
# En Windows Terminal (WSL2)
curl https://ollama.ai/install.sh | sh

# Ejecutar
ollama serve
```

### macOS

```bash
# Descargar
curl -L https://ollama.ai/download/mac -o ollama.dmg

# O descarga desde https://ollama.ai/download/mac

# Abrir e instalar
open ollama.dmg

# Ejecutar (con Spotlight)
cmd+space ‚Üí Ollama
```

### Linux

```bash
# Ubuntu/Debian
curl https://ollama.ai/install.sh | sh

# Fedora/RHEL
curl https://ollama.ai/install.sh | sh

# Ejecutar
ollama serve
```

## ü§ñ Descargar Modelos

### Desarrollo (R√°pido)
```bash
# Llama 3.1 8B - Buena relaci√≥n velocidad/calidad
ollama pull llama3.1:8b

# Tiempo: ~5 minutos
# Tama√±o: ~5GB RAM
```

### An√°lisis (Potente)
```bash
# Llama 3.1 70B - Mejor calidad (necesita 16GB+ RAM)
ollama pull llama3.1:70b

# Tiempo: ~10 minutos
# Tama√±o: ~45GB disco, 40GB RAM
```

### Alternativas Ligeras
```bash
# Mistral 7B - R√°pido y compacto
ollama pull mistral:7b

# Neural Chat - Especializado en chat
ollama pull neural-chat:7b
```

### Ver Modelos Disponibles
```bash
ollama list
```

## ‚úÖ Verificar Instalaci√≥n

### 1. Servidor corriendo
```bash
# Verifica en http://localhost:11434
curl http://localhost:11434/api/tags
```

Deber√≠a retornar:
```json
{
  "models": [
    {
      "name": "llama3.1:8b",
      "modified_at": "...",
      "size": ...
    }
  ]
}
```

### 2. Generar respuesta r√°pida
```bash
# Test simple
ollama generate --model llama3.1:8b "Hola, ¬øqui√©n eres?"

# Deber√≠a retornar respuesta en ~5 segundos
```

## üîß Configurar GestiqCloud

### 1. Copiar configuraci√≥n
```bash
cd apps/backend

# Copiar archivo de ejemplo
cp .env.ai.example .env.local

# O agregar a tu .env actual
```

### 2. Editar .env
```bash
# Aseg√∫rate que tengas:
ENVIRONMENT=development
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
OLLAMA_TIMEOUT=30
```

### 3. Verificar variables cargadas
```bash
# Desde Python
python3 -c "
import os
from dotenv import load_dotenv
load_dotenv()
print('OLLAMA_URL:', os.getenv('OLLAMA_URL'))
print('OLLAMA_MODEL:', os.getenv('OLLAMA_MODEL'))
"
```

## üß™ Probar Integraci√≥n

### 1. Test simple
```bash
# En directorio del proyecto
cd apps/backend

python3 << 'EOF'
import asyncio
from app.services.ai import AIService, AITask

async def test():
    response = await AIService.query(
        task=AITask.ANALYSIS,
        prompt="Soy un asistente de prueba. Responde con una frase corta.",
        temperature=0.3,
        max_tokens=100
    )
    
    if response.is_error:
        print(f"‚ùå Error: {response.error}")
    else:
        print(f"‚úÖ Respuesta: {response.content}")
        print(f"‚è±Ô∏è Tiempo: {response.processing_time_ms}ms")

asyncio.run(test())
EOF
```

### 2. Test en API
```bash
# Iniciar servidor (en otra terminal)
cd apps/backend
uvicorn app.main:app --reload

# En otra terminal, probar endpoint
curl -X GET http://localhost:8000/api/v1/health/ai

# Deber√≠a mostrar algo como:
# {
#   "status": "healthy",
#   "primary_provider": "ollama",
#   "providers": { "ollama": true, "ovhcloud": false, "openai": false }
# }
```

## üìä Performance Tuning

### Si Ollama es lento:

1. **Aumentar n√∫mero de threads**
```bash
# Linux/macOS
export OLLAMA_NUM_THREAD=8
ollama serve

# Windows - editar en Services
```

2. **GPU Acceleration** (si disponible)
```bash
# NVIDIA CUDA
# Ollama lo detecta autom√°ticamente

# Apple Metal (macOS)
# Ya soportado nativamente en M1+
```

3. **Memoria**
```bash
# Si falta RAM, usar modelo m√°s peque√±o
ollama pull mistral:7b  # 4.7GB vs 5GB para llama3.1:8b
```

4. **MKeep-alive** (mantener modelo en RAM)
```bash
# Agregar a curl
curl -X POST http://localhost:11434/api/generate \
  -d '{"model":"llama3.1:8b","keep_alive":"24h"}'
```

## üê≥ Alternativa: Docker

Si tienes problemas locales:

```bash
# Descargar imagen (una sola vez)
docker pull ollama/ollama

# Ejecutar contenedor
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama_data:/root/.ollama \
  ollama/ollama

# Descargar modelo dentro del contenedor
docker exec ollama ollama pull llama3.1:8b

# Usar como siempre en http://localhost:11434
```

## üìù Troubleshooting

### "Connection refused"
```bash
# 1. Verificar que Ollama est√° corriendo
ollama serve

# 2. En otra terminal, probar
curl http://localhost:11434/api/tags

# 3. Si no funciona, reiniciar:
# Windows: Services > Restart Ollama
# macOS: killall ollama && ollama serve
# Linux: systemctl restart ollama
```

### "Out of memory"
```bash
# Usar modelo m√°s peque√±o
ollama pull mistral:7b

# O aumentar RAM disponible
# Editar settings de Ollama para limitar uso
```

### Respuesta muy lenta
```bash
# 1. Verificar CPU usage
# Deber√≠a usar 6-8 cores, si usa menos aumentar OLLAMA_NUM_THREAD

# 2. Cambiar modelo
ollama pull mistral:7b  # M√°s r√°pido

# 3. Reducir max_tokens en prompts
```

### Modelo no descarga
```bash
# Verificar espacio disco
df -h

# Espacio m√≠nimo recomendado: 20GB

# Reintentar descarga
ollama pull llama3.1:8b --insecure
```

## üéØ Optimizaciones para Producci√≥n (Nota)

Para producci√≥n usa OVHCloud (ver `AI_INTEGRATION_GUIDE.md`)

Ollama es para **desarrollo local solamente** porque:
- CPU no escala bien bajo carga
- No tiene autenticaci√≥n
- No tiene rate limiting
- No es redundante

## üìö Recursos

- Documentaci√≥n Ollama: https://github.com/ollama/ollama
- Modelos disponibles: https://ollama.ai/library
- Discord community: https://discord.gg/ollama

## ‚ú® Tips

1. **Deja Ollama corriendo** en background todo el tiempo de desarrollo
2. **Cachea respuestas** para no hacer requests innecesarios
3. **Usa el modelo correcto**: 8B para desarrollo, 70B para an√°lisis profundo
4. **Keep-alive**: Ollama mantiene modelo en RAM por defecto (con timeout)
5. **Monitor**: Verifica `/api/v1/health/ai` regularmente

## üéì Pr√≥ximos Pasos

1. ‚úÖ Ollama corriendo en localhost:11434
2. ‚úÖ Configurado en .env
3. ‚úÖ Testeado con health check
4. ‚û°Ô∏è **Integrar en Copilot** (ver `COPILOT_ENHANCEMENT.md`)

---

**Tiempo total**: ~20 minutos  
**Tama√±o descarga**: ~5-10GB  
**RAM necesaria**: 8GB m√≠nimo (16GB recomendado)

¬øTienes alg√∫n problema? Revisa la secci√≥n **Troubleshooting** arriba.
