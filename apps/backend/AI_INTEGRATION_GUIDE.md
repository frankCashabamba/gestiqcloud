# GuÃ­a de IntegraciÃ³n de IA - GestiqCloud

## ğŸ“‹ Resumen

Se ha implementado una **capa de abstracciÃ³n centralizada de IA** que permite usar mÃºltiples proveedores:

- **Desarrollo**: Ollama local (gratuito, privado)
- **ProducciÃ³n**: OVHCloud AI (gestiÃ³n empresarial)
- **Fallback**: OpenAI (backup)

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Frontend (React)                 â”‚
â”‚  - Copilot, Chat, Suggestions, etc      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Backend API (FastAPI) â”‚
        â”‚   /ai/query             â”‚
        â”‚   /ai/classify          â”‚
        â”‚   /ai/analyze           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  AIProviderFactory (Router)   â”‚
        â”‚  - Selecciona proveedor       â”‚
        â”‚  - Maneja fallback            â”‚
        â”‚  - Health checks              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚              â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Ollama â”‚          â”‚ OVHCloud  â”‚   â”‚  OpenAI  â”‚
    â”‚ Local  â”‚          â”‚ Manager   â”‚   â”‚   API    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Desarrollo Local con Ollama

#### 1. Instalar Ollama
```bash
# macOS/Linux
curl https://ollama.ai/install.sh | sh

# o descarga desde https://ollama.ai
```

#### 2. Descargar modelo
```bash
ollama pull llama3.1:8b
# O para anÃ¡lisis mÃ¡s potente:
ollama pull llama3.1:70b
```

#### 3. Verificar que corre
```bash
curl http://localhost:11434/api/tags
```

#### 4. Configurar .env
```bash
ENVIRONMENT=development
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
OLLAMA_TIMEOUT=30
OLLAMA_HEALTH_TIMEOUT=5
```

#### 5. Listo
Desde el backend, ahora todas las consultas de IA usan Ollama local.

### ProducciÃ³n con OVHCloud

#### 1. Configurar credenciales
```bash
ENVIRONMENT=production
OVHCLOUD_API_URL=https://manager.eu.ovhcloud.com/api/v2
OVHCLOUD_API_KEY=your_api_key_here
OVHCLOUD_API_SECRET=your_api_secret_here
OVHCLOUD_MODEL=gpt-4o
OVHCLOUD_SERVICE_NAME=ai
OVHCLOUD_TIMEOUT=60
```

#### 2. Configurar OpenAI como fallback
```bash
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo
```

#### 3. Verificar salud
```bash
curl http://localhost:8000/api/v1/health/ai
```

## ğŸ“š Uso en el CÃ³digo

### Importar
```python
from app.services.ai import AIProviderFactory, AIService, AITask

# Inicializar factory
AIProviderFactory.initialize()
```

### Ejemplo 1: Consulta GenÃ©rica
```python
from app.services.ai import AIService, AITask

response = await AIService.query(
    task=AITask.ANALYSIS,
    prompt="Analiza estos nÃºmeros de ventas: ...",
    temperature=0.3,
    max_tokens=1000
)

if response.is_error:
    print(f"Error: {response.error}")
else:
    print(response.content)
    print(f"Tokens usados: {response.tokens_used}")
```

### Ejemplo 2: Clasificar Documento
```python
result = await AIService.classify_document(
    document_content="FACTURA #001...",
    expected_types=["invoice", "receipt", "order", "transfer"],
    confidence_threshold=0.7
)

print(f"Tipo: {result['type']}")
print(f"Confianza: {result['confidence']:.1%}")
print(f"Requiere revisiÃ³n: {result['requires_review']}")
```

### Ejemplo 3: Generar Sugerencias
```python
suggestion = await AIService.generate_suggestion(
    context="Stock bajo de producto XYZ, tendencia de ventas creciente",
    suggestion_type="inventory"
)
print(suggestion)  # "Considerar aumentar pedido de..."
```

### Ejemplo 4: Analizar Incidencia
```python
analysis = await AIService.analyze_incident(
    incident_type="database_error",
    description="Connection timeout despuÃ©s de 30 segundos",
    stack_trace="...",
    additional_context={
        "module": "sales",
        "frequency": "intermittent",
        "environment": "production"
    }
)

print(f"Causa raÃ­z: {analysis['root_cause']}")
print(f"Acciones recomendadas: {analysis['recommended_actions']}")
```

### Ejemplo 5: Generar Borrador de Documento
```python
draft = await AIService.generate_document_draft(
    document_type="invoice",
    data={
        "client_name": "Empresa XYZ",
        "items": [
            {"description": "Servicio A", "qty": 2, "unit_price": 100}
        ],
        "subtotal": 200,
        "iva": 21,
    }
)

print(f"Contenido: {draft['content']}")
print(f"Campos: {draft['fields']}")
if draft['warnings']:
    print(f"Advertencias: {draft['warnings']}")
```

## ğŸ”Œ IntegraciÃ³n en MÃ³dulos Existentes

### 1. Copilot Mejorado
```python
# apps/backend/app/modules/copilot/services.py
from app.services.ai import AIService, AITask

async def enhance_query_with_ai(query_results: list[dict]) -> str:
    """Mejora resultados de query con anÃ¡lisis IA"""
    summary = json.dumps(query_results, indent=2)
    response = await AIService.query(
        task=AITask.ANALYSIS,
        prompt=f"Resume y da insights de estos datos:\n{summary}"
    )
    return response.content
```

### 2. ClasificaciÃ³n de Documentos (Imports)
```python
# apps/backend/app/modules/imports/interface/http/tenant.py
from app.services.ai import AIService

result = await AIService.classify_document(
    document_content=extracted_text,
    expected_types=["invoice", "purchase_order", "receipt"],
    confidence_threshold=0.75
)

if result['requires_review']:
    # Enviar a revisiÃ³n manual
    pass
else:
    # Procesar automÃ¡ticamente
    process_document(result['type'])
```

### 3. AnÃ¡lisis de Incidencias
```python
# apps/backend/app/routers/incidents.py
from app.services.ai import AIService

analysis = await AIService.analyze_incident(
    incident_type=incident.tipo,
    description=incident.description,
    stack_trace=incident.stack_trace
)

incident.ai_analysis = analysis
db.commit()
```

### 4. Chat Inteligente (Nuevo mÃ³dulo)
```python
# apps/backend/app/modules/copilot/chat.py
from app.services.ai import AIService, AITask

response = await AIService.query(
    task=AITask.CHAT,
    prompt=user_message,
    context={
        "module": current_module,
        "user_role": user_role,
        "company_data": aggregated_data
    }
)
```

### 5. Sugerencias Contextuales
```python
# apps/backend/app/modules/sales/suggestions.py
from app.services.ai import AIService

suggestion = await AIService.generate_suggestion(
    context=f"""
    Cliente: {client.name}
    Ãšltimo pedido: {last_order.date}
    PatrÃ³n de compra: {buying_pattern}
    Stock disponible: {available_stock}
    """,
    suggestion_type="upsell"
)
```

## âš™ï¸ ConfiguraciÃ³n de Entorno

### Variables MÃ­nimas Requeridas

#### Desarrollo
```bash
ENVIRONMENT=development
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
```

#### ProducciÃ³n
```bash
ENVIRONMENT=production
OVHCLOUD_API_KEY=...
OVHCLOUD_API_SECRET=...
OPENAI_API_KEY=...  # Fallback
```

### Variables Opcionales
```bash
# LÃ­mites y timeouts
AI_MAX_PROMPT_LENGTH=10000
OLLAMA_TIMEOUT=30
OVHCLOUD_TIMEOUT=60
OPENAI_TIMEOUT=30

# Modelo especÃ­fico
OLLAMA_MODEL=llama3.1:8b
OVHCLOUD_MODEL=gpt-4o
OPENAI_MODEL=gpt-3.5-turbo
```

## ğŸ“Š Monitoreo y Debugging

### Health Check
```python
from app.services.ai import AIProviderFactory

# Revisar todos los proveedores
status = await AIProviderFactory.health_check_all()
# {'ollama': True, 'ovhcloud': False, 'openai': True}
```

### Logs
```python
import logging

# Habilitar logs detallados de IA
logging.getLogger("app.services.ai").setLevel(logging.DEBUG)
```

### Endpoint de Health
```
GET /api/v1/health/ai
```

Retorna:
```json
{
  "primary_provider": "ollama",
  "available_providers": {
    "ollama": true,
    "ovhcloud": false,
    "openai": true
  },
  "current_models": {
    "ollama": "llama3.1:8b",
    "openai": "gpt-3.5-turbo"
  }
}
```

## ğŸ§ª Testing

### Mock Provider para Tests
```python
@pytest.fixture
async def mock_ai():
    from app.services.ai.base import AIResponse, AITask

    async def mock_call(request):
        return AIResponse(
            task=request.task,
            content="Mock response",
            model="mock",
            tokens_used=10
        )

    return mock_call

async def test_classification(mock_ai):
    result = await AIService.classify_document(
        "Test doc",
        ["type1", "type2"]
    )
    assert result['type'] in ["type1", "type2"]
```

## ğŸ” Consideraciones de Seguridad

### Privacidad
- **Ollama (Desarrollo)**: Completamente local, sin datos externos
- **OVHCloud (ProducciÃ³n)**: Datos encriptados en trÃ¡nsito, cumple GDPR
- **OpenAI (Fallback)**: Revisar polÃ­ticas de privacidad antes de usar en producciÃ³n

### Rate Limiting
```python
# Implementado a nivel de API
# Ver app/middleware/rate_limit.py
```

### ValidaciÃ³n de Prompts
```python
# SanizaciÃ³n automÃ¡tica en BaseAIProvider._prepare_prompt()
- MÃ¡ximo de caracteres: 10,000
- DetecciÃ³n de inyecciÃ³n SQL (futuro)
- Filtrado de datos sensibles (futuro)
```

## ğŸ“¦ Dependencias Nuevas

Agregar a `requirements.txt`:
```
httpx>=0.28.1  # Ya existe
```

No se requieren nuevas dependencias pesadas.

## ğŸ¯ PrÃ³ximos Pasos

1. **Integrar en Copilot** (fase inmediata)
   - Mejorar respuestas con anÃ¡lisis IA
   - Agregar chat conversacional

2. **Integrar en Imports** (fase 2)
   - Reemplazar Ollama directo con factory
   - Mejorar clasificaciÃ³n con fallback

3. **Integrar en Incidents** (fase 2)
   - Usar nuevo AIService.analyze_incident()

4. **Nuevo mÃ³dulo Chat** (fase 3)
   - Chat inteligente en todos los mÃ³dulos
   - Soporte para context-aware responses

5. **Dashboard de IA** (fase 3)
   - Monitoreo de proveedores
   - Uso de tokens y costos
   - Sugerencias recientes

## ğŸ’¡ Tips

- MantÃ©n temperatura baja (0.1-0.3) para tareas determinÃ­sticas (clasificaciÃ³n, anÃ¡lisis)
- Usa temperatura media (0.5-0.7) para generaciÃ³n creativa
- Siempre maneja `response.is_error` para fallos graceful
- Cachea respuestas IA cuando sea posible
- Usa context para mejorar relevancia

## ğŸ“ Soporte

Para preguntas sobre integraciÃ³n de IA:
1. Revisa logs en `DEBUG` level
2. Comprueba health en `/api/v1/health/ai`
3. Valida configuraciÃ³n en `.env`
4. Prueba con curl directamente al proveedor
